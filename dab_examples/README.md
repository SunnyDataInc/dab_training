# dab_examples
The 'dab_examples' project was generated by using the Databricks Asset Bundles **default-python** template. It contains a series of databricks jobs deployments that range from basic serveless notebook jobs to orchestrations where a job triggers another job.

## Jobs details:
All the jobs managed by this bundle find their base configuration in the file **databricks.yml**

### Simple_Serverless
Serverless Compute job that contains only one notebook task.<br>
Files:
```
dab_examples
│  README.md
│  databricks.yml    
└─ jobs
│  └─ simple_serverless.job.yml
└─ notebooks
   └─ notebook.ipynb
```
### Simple_Parameter
Serverless Compute job with only one notebook task that ingests a job parameter (string). It adds to the string and then creates a task value that can be shared with other notebooks later on.<br>
Files:
```
dab_examples
│  README.md
│  databricks.yml    
└─ jobs
│  └─ simple_parameter.job.yml
└─ notebooks
   └─ notebook_parameter.ipynb
```
### Double_Task_Serverless
Serverless Compute job that runs two notebooks in sequence.<br>
Files:
```
dab_examples
│  README.md
│  databricks.yml    
└─ jobs
│  └─ double_task_serverless.job.yml
└─ notebooks
   └─ notebook.ipynb
   └─ notebook_2.ipynb
```
### Double_Task_Diff_Clusters
This job uses two notebooks that run in sequence: The first notebook uses a job cluster (usefull when there are special cluster configurations needed) and the second job uses an existing all-purpose cluster (This is never recommended, this is just an example).<br>
Files:
```
dab_examples
│  README.md
│  databricks.yml    
└─ jobs
│  └─ double_task_serverless.job.yml
└─ notebooks
│  └─ notebook_2.ipynb
│  └─ notebook_3.ipynb
└─ clusters
   └─ dab_clusters.yml
```
### Passing_Parameters_Between_Tasks
Serverless job that uses two notebooks that run in sequence: The first notebook ingests a job parameter, transforms it and then creates a task value to share downstream. The second job ingests the task value from the first notebook as a task parameter.<br>
Files:
```
dab_examples
│  README.md
│  databricks.yml    
└─ jobs
│  └─ passing_parameters_between_tasks.job.yml
└─ notebooks
   └─ notebook_parameter.ipynb
   └─ notebook_task_parameter.ipynb
```
### Passing_Parameters_Between_Jobs
Orchestration of two jobs, both with a single notebook task. The first job ingests a job parameter, transforms it and then creates a task value to share downstream.
The second job ingests the task value from the first job as a job parameter.<br>
Files:
```
dab_examples
│  README.md
│  databricks.yml    
└─ jobs
│  └─ passing_parameters_between_jobs.job.yml (job 1)
│  └─ job_2.job.yml (job 2)
└─ notebooks
   └─ notebook_parameter.ipynb (used by job 1)
   └─ notebook_job_parameter.ipynb (used by job 2)
```
### Conditional_Dependency_Task_Serverless
This job have three notebook-tasks, where the 3rd task runs if **at least** one of its parent notebook-tasks succeeds.<br>
Files:
```
dab_examples
│  README.md
│  databricks.yml    
└─ jobs
│  └─ conditional_dependency_task_serverless.job.yml
└─ notebooks
   └─ notebook.ipynb
   └─ notebook_2.ipynb
   └─ notebook_3.ipynb
```
### Cluster_Parameters
This job uses variables **defined** in the **databricks.yml** file for the notebook path, as well as for identifying the cluster it needs to use. Also, in the cluster configuration we defined a spark configuration variable and a spark environment variable.<br>
Files:
```
dab_examples
│  README.md
│  databricks.yml    
└─ jobs
│  └─ cluster_parameters.job.yml
└─ notebooks
   └─ reading_spark_parameters.ipynb (as defined in the dab_notebook variable)
```

## Getting started

1. Install the Databricks CLI from https://docs.databricks.com/dev-tools/cli/databricks-cli.html

2. Authenticate to your Databricks workspace, if you have not done so already:
    ```
    $ databricks configure
    ```

3. To deploy a development copy of this project, type:
    ```
    $ databricks bundle deploy --target dev
    ```
    (Note that "dev" is the default target, so the `--target` parameter
    is optional here.)

    This deploys everything that's defined for this project.
    For example, the default template would deploy a job called
    `[dev yourname] simple_serverless_job` to your workspace.
    You can find that job by opening your workpace and clicking on **Workflows**.

4. Similarly, to deploy a production copy, type:
   ```
   $ databricks bundle deploy --target prod
   ```

   Note that the default job from the template has a schedule that runs every day
   (defined in resources/init_project.job.yml). The schedule
   is paused when deploying in development mode (see
   https://docs.databricks.com/dev-tools/bundles/deployment-modes.html).

5. To run a job or pipeline, use the "run" command:
   ```
   $ databricks bundle run
   ```
6. Optionally, install developer tools such as the Databricks extension for Visual Studio Code from
   https://docs.databricks.com/dev-tools/vscode-ext.html.

7. For documentation on the Databricks asset bundles format used
   for this project, and for CI/CD configuration, see
   https://docs.databricks.com/dev-tools/bundles/index.html.